{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e7877d",
   "metadata": {},
   "source": [
    "# Assignmnet 2 (100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b6d20",
   "metadata": {},
   "source": [
    "**Name: Daniel Shaquille** <br>\n",
    "**Email: das9688@thi.de** <br>\n",
    "**Group:** B <br>\n",
    "**Hours spend *(optional)* :** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f79f88",
   "metadata": {},
   "source": [
    "### SMS Spam Detection *(100 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e88d0",
   "metadata": {},
   "source": [
    "<p>You are hired as an AI expert in the development department of a telecommunications company. The first thing on your orientation plan is a small project that your boss has assigned you for the following given situation. Your supervisor has given away his private cell phone number on too many websites and is now complaining about daily spam SMS. Therefore, it is your job to write a spam detector in Python. </p>\n",
    "\n",
    "<p>In doing so, you need to use a Naive Bayes classifier that can handle both bag-of-words (BoW) and tf-idf features as input. For the evaluation of your spam detector, an SMS collection is available as a dataset - this has yet to be suitably split into train and test data. To keep the costs as low as possible and to avoid problems with copyrights, your boss insists on a new development with Python.</p>\n",
    "\n",
    "<p>Include a short description of the data preprocessing steps, method, experiment design, hyper-parameters, and evaluation metric. Also, document your findings, drawbacks, and potential improvements.</p>\n",
    "\n",
    "<p>Note: You need to implement the bag-of-words (BoW) and tf-idf feature extractor from scratch. You can use existing python libraries for other tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad12eba",
   "metadata": {},
   "source": [
    "**Dataset and Resources**\n",
    "\n",
    "* SMS Spam Collection Dataset: https://archive.ics.uci.edu/dataset/228/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4109920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b6897",
   "metadata": {},
   "source": [
    "<h1>Bag of Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa47543",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "\n",
    "x = data[1]\n",
    "y = data[0]\n",
    "\n",
    "data = x.tolist()\n",
    "\n",
    "\n",
    "# Now 'data' is a list containing only the messages\n",
    "original_data = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bag Of Words'''\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "def custom_fit(data):\n",
    "\n",
    "    unique_words = set()\n",
    "\n",
    "    for each_sentence in data:\n",
    "        for each_word in each_sentence.split(' '):\n",
    "\n",
    "            if len(each_word) > 2:\n",
    "                unique_words.add(each_word)\n",
    "\n",
    "    # Getting the index of the word\n",
    "    vocab = {}\n",
    "\n",
    "    for i, w in enumerate(sorted(list(unique_words))):\n",
    "        vocab[w] = i\n",
    "    print(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Get vocabulary index\n",
    "def custom_transform(data):\n",
    "    vocab = custom_fit(data)  \n",
    "\n",
    "    \n",
    "    num_docs = len(data)\n",
    "    num_words = len(vocab)\n",
    "    bow_matrix = np.zeros((num_docs, num_words), dtype=int)\n",
    "\n",
    "    \n",
    "    for i, doc in enumerate(data):\n",
    "        word_counts = {}\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            # Filter out short words\n",
    "            if len(word) > 2:  \n",
    "                if word in vocab:\n",
    "                    col_index = vocab[word] # get the col index of word from vocab\n",
    "                    if col_index not in word_counts:\n",
    "                        word_counts[col_index] = 0\n",
    "                    word_counts[col_index] += 1\n",
    "\n",
    "        # Update the corresponding entries in the bag-of-words matrix\n",
    "        for col_index, count in word_counts.items():\n",
    "            bow_matrix[i, col_index] = count\n",
    "\n",
    "    return bow_matrix\n",
    "\n",
    "bag_of_words = custom_transform(original_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_BoW, X_test_BoW, y_train_BoW, y_test_BoW = train_test_split(bag_of_words, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_BoW, y_train_BoW)\n",
    "y_pred = classifier.predict(X_test_BoW)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_BoW, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bag of Words'''\n",
    "\n",
    "''' \n",
    "For preprocessing, the custom fit function will take words and fit them by giving index\n",
    "We go through the whole data through each sentences and will give a count for the words\n",
    "We then sort and index them\n",
    "\n",
    "Then we will get the count values for each word\n",
    "We then are filling the counted value of each row in the matrix and other values in that row are zero\n",
    "Initialize an empty matrix to hold the bag-of-words representation\n",
    "Iterate over each document and update the bag-of-words matrix\n",
    "\n",
    "After we have obtained our bag of words, we split the data into train and test where our test data size is 20 percent pf the whole dataset\n",
    "We then pass our training dataset into the MultinomialNB(), then get the prediction using the test dataset.\n",
    "Finally we get our accuracy score\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa43f0",
   "metadata": {},
   "source": [
    "<h1>TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240db101",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "# print(data)\n",
    "\n",
    "x = data[1]\n",
    "y = data[0]\n",
    "\n",
    "data = x.tolist()\n",
    "\n",
    "# Now 'data' is a list containing only the messages\n",
    "original_data2 = data\n",
    "print(original_data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for preprocessing\n",
    "def preprocess_text(text):\n",
    "    words_list = []\n",
    "    for sent in text:\n",
    "        words = [word.lower() for word in sent.split() if word.isalpha()]\n",
    "        words_list.append(words)\n",
    "    return words_list\n",
    "\n",
    "sentences = preprocess_text(original_data2)\n",
    "\n",
    "\n",
    "word_set = set(word for words in sentences for word in words)\n",
    "# print(word_set)\n",
    "\n",
    "total_docs = len(original_data2)\n",
    "\n",
    "\n",
    "# We index each word from the vocabulary to map the word to the vector\n",
    "word_index = {}\n",
    "for i, w in enumerate(word_set):\n",
    "    word_index[w] = i\n",
    "    \n",
    "\n",
    "# A function to keep count of the numbers of documents containing the word\n",
    "def count_dict(sentences):\n",
    "    count_dict = {}\n",
    "    # print(word_set)\n",
    "    for word in word_set:\n",
    "        count_dict[word] = 0\n",
    "    \n",
    "    # print(sentences)\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            count_dict[word] += 1\n",
    "    \n",
    "    return count_dict\n",
    "\n",
    "word_count = count_dict(sentences)\n",
    "# print(word_count)\n",
    "\n",
    "\n",
    "# Term Frequency calculation\n",
    "def term_frequency(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    # print(occurance)\n",
    "    # print(occurance/N)\n",
    "    return occurance/N\n",
    "\n",
    "# Inverse Document Frequency Calculation\n",
    "def inverse_document_frequency(word):\n",
    "    # print(word)\n",
    "    try:\n",
    "        word_occurance = word_count[word] +1\n",
    "    \n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    \n",
    "    return np.log(total_docs/ word_occurance)\n",
    "\n",
    "# TF-IDF function\n",
    "def tf_idf(sentence):\n",
    "    vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = term_frequency(sentence, word)\n",
    "        idf = inverse_document_frequency(word)\n",
    "        vec[word_index[word]] = tf * idf\n",
    "    \n",
    "    return vec\n",
    "\n",
    "\n",
    "tf_idf_converted = []\n",
    "for sent in sentences:\n",
    "    tf_idf_converted.append(tf_idf(sent))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec68d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tf_idf_converted, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Goaussian and Binomial\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_tfidf, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TF-IDF'''\n",
    "\n",
    "'''  \n",
    "First we preprocess and tokenize the text\n",
    "We then have a set of unique words in the corpus\n",
    "Index each word in the vocabulary\n",
    "Create a function named count_dict() to keep count of the numbers of documents containing the word\n",
    "We create a function to calculate the term frequency of a word in a document\n",
    "We also create a function to calculate the inverse document frequency of a word\n",
    "Finally we have also a function to calculate the TF-IDF vector for a sentence and we will get arrays of our tf-idf value dataset\n",
    "\n",
    "After we have obtained data of array of tf-idf, we split the data into train and test where our test data size is 20 percent pf the whole dataset\n",
    "We then pass our training dataset into the MultinomialNB(), then get the prediction using the test dataset.\n",
    "Finally we get our accuracy score\n",
    "\n",
    "Findings are in this case that the bag of words gives a higher accuracy score compared with the tf-idf\n",
    "Some drawbacks are in the preprocessing part to clean the data\n",
    "For potential improvements, we could use the nltk libraries like word_tokenize or stopwords and also use Countvectorizer\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbc82a",
   "metadata": {},
   "source": [
    "### Additional Experiments *(5 additional points - <span style=\"color: red;\">Optional</span>)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5820d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, encoding='ISO-8859-1')\n",
    "# print(data)\n",
    "\n",
    "x = data[1]\n",
    "y = data[0]\n",
    "\n",
    "data = x.tolist()\n",
    "\n",
    "# Now 'data' is a list containing only the messages\n",
    "original_data3 = data  \n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    tokens = [token for token in tokens if token.isalpha()]  \n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')] \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "processed_texts = [preprocess_text(text) for text in original_data3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07712d34",
   "metadata": {},
   "source": [
    "<h1>TF_IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d1734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0432a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d7d97",
   "metadata": {},
   "source": [
    "<h1>Bag of Words</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07571f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
